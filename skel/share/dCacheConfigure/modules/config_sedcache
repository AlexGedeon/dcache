# New Yaim Configuration and setup for D-Cache 1.6.6-X 1.7.0, 1.7.1, and 1.8.0
# Maintained by Owen Synge on behalf of Dcache
# Based upon work by 
# M Litmaath, M de Riese, J Mencak, J Novak,L Poncent, O Keeble, L Field and M Aart
# which based thier work upon d-cache configuration adapted 
# from package d-cache-lcg

# GridPP's testing has been very useful

# This script is based upon earlier work by LHC


# Used variables
# Obligatory: DCACHE_ADMIN=admin_host
#             DCACHE_POOLS="pool_node1:[size:]/pool_path1 pool_node2:[size:]/pool_path2"
#             <size> ::= [0-9]*
# Optional:   DCACHE_PNFS_SERVER=pnfs_host
# Optional:   DCACHE_PORT_RANGE="20000,25000"
# Optional:   DCACHE_DOOR_SRM="pool_node1[:port]"
#          Defaults to Admin nodes
# Optional:   DCACHE_DOOR_GSIFTP="pool_node1[:port] pool_node2[:port]"
#          Defaults to Door nodes
# Optional:   DCACHE_DOOR_GSIDCAP="pool_node1[:port] pool_node2[:port]"
#          Defaults to Door nodes
# Optional:   DCACHE_DOOR_DCAP="pool_node1[:port] pool_node2[:port]"
#          Defaults to Door nodes


# Set this to wipe the postgres databases
# Optional:RESET_DCACHE_RDBMS=yes
#
# Set this to reset the PNFS server
# Optional:RESET_DCACHE_PNFS=yes
#
# Set this to reset the D-cache configuration
# Optional:RESET_DCACHE_CONFIGURATION=yes



# Structure of this script

# functions starting yaim_query_conf

# This script was always designed as a way to make modification easy
# To this end set up questions are placed at the top of the file. This
# is intended to make porting Yaim to debconf or any other cluster wide 
# management system easy. These functions are queries that could in 
# practice be queries to an interactive installer or a cluster 
# management system

#
# functions starting yaim_state

# These functions are to establish the state of the install and direct 
# what components are to be installed. They do not change the state 
# of uninstalled/installed components


# functions starting yaim_config


## Start of yaim query functions





yaim_query_conf_node_dcache_admin()
{
    # Returns 0 when node is not admin node
    # Returns 1 when node is admin node
    yaimlog DEBUG "function yaim_query_conf_node_dcache_admin start"
    local result
    local thishost
    #Set default result
    result=0
    if [ -n "${DCACHE_ADMIN}" ] ; then
        #Get Host name
        thishost=`hostname -f`
        if [ "x${DCACHE_ADMIN#$thishost}" == "x" ]; then
            result=1
        fi
    fi
    yaimlog DEBUG "function yaim_query_conf_node_dcache_admin stop"
    return ${result}
}


yaim_query_conf_node_dcache_name_server()
{
    # Returns 0 when node is not a nameserver node
    # Returns 1 when node is a nameserver node
    local result
    local thishost
    # badnessCount is used to count duplicate variables
    local badnessCount
    local nameserverhostname
    local usingChimera
    local usingPNFS
    local usingNamesSpace
    result=0
    badnessCount=0
    
    if [ -z "${DCACHE_NAME_SERVER}${DCACHE_CHIMERA_SERVER}${DCACHE_PNFS_SERVER}" ] ; then
        # No we know that the name server is not defined
        yaim_query_conf_node_dcache_admin
        result=$?
    else
        
        if [ -n "${DCACHE_NAME_SERVER}" ] ; then
            nameserverhostname=${DCACHE_NAME_SERVER}
            usingNamesSpace=1
        fi
        if [ -n "${DCACHE_CHIMERA_SERVER}" ] ; then
            nameserverhostname=${DCACHE_CHIMERA_SERVER}
            usingChimera=1
        fi
        if [ -n "${DCACHE_PNFS_SERVER}" ] ; then
            nameserverhostname=${DCACHE_PNFS_SERVER}
            usingPNFS=1
        fi
        if [ "11" == "${usingPNFS}${usingChimera}" ]  ; then
            yaimlog ERROR "Both variables DCACHE_PNFS_SERVER and DCACHE_CHIMERA_SERVER are specified."
            exit 1
        fi
        if [ -n "${DCACHE_PNFS_SERVER}${DCACHE_CHIMERA_SERVER}" ] ; then
            if [ "${nameserverhostname}" != "${DCACHE_PNFS_SERVER}${DCACHE_CHIMERA_SERVER}" ] ; then
                yaimlog ERROR "DCACHE_NAME_SERVER is set and does not match the variables DCACHE_PNFS_SERVER or DCACHE_CHIMERA_SERVER."
            echo nameserverhostname=${nameserverhostname}
                exit 1
            fi
        fi
        
        thishost=`hostname -f`
        hostspec=$(echo "$nameserverhostname" | grep -o "$thishost[^ ]*")
        if [ "x$hostspec" != "x" ]; then
            result=1
        fi
    fi
    return ${result}
}
# Returns List of chimera servers. Failing that PNFS servers
# Failing that a list containing just the admin node
# This should only ever be one host, but is a "list" to be consistent
yaim_query_conf_node_dcache_name_server_list()
{
    local result
    local thishost
    RET=""
    if [ -z "$DCACHE_CHIMERA_SERVER" ] ; then
        if [ -z "$DCACHE_PNFS_SERVER" ] ; then
	    RET=$DCACHE_ADMIN
        else
            RET="$DCACHE_PNFS_SERVER"
        fi
    else
        RET="$DCACHE_CHIMERA_SERVER"
    fi
}

yaim_query_conf_node_dcache_name_type()
{
    local result
    yaim_query_conf_node_dcache_pnfsserver_list
    if [ -n "${RET}" ] ; then
         result="pnfs"
    fi   
    yaim_query_conf_node_dcache_chimeraserver_list
    if [ -n "${RET}" ] ; then
         result="chimera"
    fi
    RET=${result}
}


yaim_query_conf_node_dcache_pool()
{
    # Returns 0 when node is not pool node
    # Returns 1 when node is pool node
    local result
    local thishost
    #Set default result
    result=0
    #Get Host name
    thishost=`hostname -f`
    poolspec=`echo "$DCACHE_POOLS" | grep -o "$thishost:[^ ]*"`
    if [ "x$poolspec" != "x" ]; then
            result=1
    fi
    return ${result}
}


yaim_query_conf_dcache_pool_node_list()
{
    RET=""
    for thishost in ${DCACHE_POOLS}
    do
        RET="$(echo $thishost | cut -d: -f1 ) $RET" 
    done
}



yaim_query_conf_node_dcache_nameserver_user()
{
    # Returns 0 when node is not srm_door node
    # Returns 1 when node is srm_door node
    local resultset
    local result
    resultset=""
    result=0    
    yaim_query_conf_node_dcache_admin
    resultset=${resultset}$?
    yaim_query_conf_node_dcache_name_server
    resultset=${resultset}$?
    yaim_query_conf_node_dcache_door
    resultset=${resultset}$?
    if [ -n "$(echo $resultset | grep 1)" ] ; then
        result=1
    fi
    return ${result}
}

# Takes parameters of hostname optionaly 
# if not specified uses local host
yaim_query_conf_dcache_pool_node_size_path()
{
    tempval=""
    RET=""
    HOST=$1
    if [ -z "$HOST" ] ; then
        HOST=`hostname -f`
    fi
    for thishost in ${DCACHE_POOLS}
    do
        if [ ! -z "$(echo $thishost | grep $HOST )" ] ; then
            if [ ! -z "$(echo $thishost | grep : )" ] ; then
                RET="$RET $(echo ${thishost#*:} )"
            fi
        fi
    done
}





yaim_query_conf_dcache_portrange()
{
yaimlog DEBUG "function yaim_query_conf_dcache_portrange start"
local lower
local upper
# Default port_range
RET=50000,52000

if [ "x$DCACHE_PORT_RANGE" != "x" ]; then
    lower=`echo "$DCACHE_PORT_RANGE" | sed -e 's/^\([0-9]*\),.*$/\1/' -e 's/.*,.*//'`
    upper=`echo "$DCACHE_PORT_RANGE" | sed -e 's/^.*,\([0-9]*\)$/\1/' -e 's/.*,.*//'`

    case $lower,$upper in
    ????*,????*)
        RET="$lower,$upper"
        ;;
    *)
        echo "[ERROR]: \$DCACHE_PORT_RANGE has illegal format:"
        echo "[ERROR]: $DCACHE_PORT_RANGE"
        return 2
    esac
fi
yaimlog DEBUG "function yaim_query_conf_dcache_portrange stop"
return 0
}




yaim_query_conf_dcache_portrange_gsiftp_server()
{
yaimlog DEBUG "function yaim_query_conf_dcache_portrange_gsiftp_server start"
local lower
local upper
# Default port_range
RET=50000,52000

if [ "x$DCACHE_PORT_RANGE_PROTOCOLS_SERVER_GSIFTP" != "x" ]; then
    lower=`echo "$DCACHE_PORT_RANGE_PROTOCOLS_SERVER_GSIFTP" | sed -e 's/^\([0-9]*\),.*$/\1/' -e 's/.*,.*//'`
    upper=`echo "$DCACHE_PORT_RANGE_PROTOCOLS_SERVER_GSIFTP" | sed -e 's/^.*,\([0-9]*\)$/\1/' -e 's/.*,.*//'`

    case $lower,$upper in
    ????*,????*)
        RET="$lower,$upper"
        ;;
    *)
        echo "[ERROR]: \$DCACHE_PORT_RANGE_PROTOCOLS_SERVER_GSIFTP has illegal format:"
        echo "[ERROR]: $DCACHE_PORT_RANGE_PROTOCOLS_SERVER_GSIFTP"
        return 2
    esac
fi
yaimlog DEBUG "function yaim_query_conf_dcache_portrange_gsiftp_server stop"
return 0
}

yaim_query_conf_dcache_portrange_gsiftp_client()
{
yaimlog DEBUG "function yaim_query_conf_dcache_portrange_gsiftp_client start"
local lower
local upper
# Default port_range
RET=60000,62000

if [ "x$DCACHE_PORT_RANGE_PROTOCOLS_CLIENT_GSIFTP" != "x" ]; then
    lower=`echo "$DCACHE_PORT_RANGE_PROTOCOLS_CLIENT_GSIFTP" | sed -e 's/^\([0-9]*\),.*$/\1/' -e 's/.*,.*//'`
    upper=`echo "$DCACHE_PORT_RANGE_PROTOCOLS_CLIENT_GSIFTP" | sed -e 's/^.*,\([0-9]*\)$/\1/' -e 's/.*,.*//'`

    case $lower,$upper in
    ????*,????*)
        RET="$lower,$upper"
        ;;
    *)
        echo "[ERROR]: \$DCACHE_PORT_RANGE_PROTOCOLS_CLIENT_GSIFTP has illegal format:"
        echo "[ERROR]: $DCACHE_PORT_RANGE_PROTOCOLS_CLIENT_GSIFTP"
        return 2
    esac
fi
yaimlog DEBUG "function yaim_query_conf_dcache_portrange_gsiftp_client stop"
return 0
}



yaim_query_conf_dcache_portrange_protocols_native()
{
yaimlog DEBUG "function yaim_query_conf_dcache_portrange_protocols_native start"
local lower
local upper
# Default port_range
RET=33115,33215

if [ "x$DCACHE_PORT_RANGE_PROTOCOLS_SERVER_MISC" != "x" ]; then
    lower=`echo "$DCACHE_PORT_RANGE_PROTOCOLS_SERVER_MISC" | sed -e 's/^\([0-9]*\),.*$/\1/' -e 's/.*,.*//'`
    upper=`echo "$DCACHE_PORT_RANGE_PROTOCOLS_SERVER_MISC" | sed -e 's/^.*,\([0-9]*\)$/\1/' -e 's/.*,.*//'`

    case $lower,$upper in
    ????*,????*)
        RET="$lower,$upper"
        ;;
    *)
        echo "[ERROR]: \$DCACHE_PORT_RANGE_PROTOCOLS_SERVER_MISC has illegal format:"
        echo "[ERROR]: $DCACHE_PORT_RANGE_PROTOCOLS_SERVER_MISC"
        return 2
    esac
fi
yaimlog DEBUG "function yaim_query_conf_dcache_portrange_protocols_native stop"
return 0
}


yaim_query_conf_node_srm_door()
{
  # Returns 0 when node is not srm_door node
  # Returns 1 when node is srm_door node
  local result
  local thishost
  local hostspec
  result=0
  if [ -z "$DCACHE_DOOR_SRM" ] ; then
    yaim_query_conf_node_dcache_admin
    result=$?
  else
    thishost=`hostname -f`
    hostspec=$(echo "$DCACHE_DOOR_SRM" | grep -o "$thishost[^ ]*")
    if [ "x$hostspec" != "x" ]; then
      result=1
    fi
  fi
  return ${result}
}


#Returns a string of all host:port pairs that the configuration 
#will give cluster wide

yaim_query_conf_dcache_srm_door_node_port_list()
{
  local hostlist
  local thishost
  local thisport
  local thishostport
  thishost=`hostname -f`
  if [ -z "${DCACHE_DOOR_SRM}" ] ; then
    RET=${DCACHE_ADMIN}:8443
  else
    hostlist=$DCACHE_DOOR_SRM
    RET=""
    for thishostport in ${hostlist}
    do
      thishost=$(echo ${thishostport} | cut -d: -f1 )
      thisport=$(echo ${thishostport} | cut -s -d: -f2 )
      if [ -z "${thisport}" ] ; then
        thisport=8443
      fi
      RET="$thishost:$thisport $RET"
    done
  fi
}



yaim_query_conf_node_srm_door_port()
{
    # Returns 0 when node is not a gsidcap node
    # Returns 1 when node is a gsidcap node
    local thishost
    local NodePortList
    thishost=`hostname -f`
    yaim_query_conf_dcache_srm_door_node_port_list
    NodePortList=$RET
    RET=""
    for ahost in ${NodePortList}
    do
        if [ "${thishost}" == "${ahost:0:${#thishost}}" ] ; then 
            RET="${ahost:${#thishost} + 1}"
            break
        fi
    done
    if [ -z "$RET" ] ; then
        RET=8443
    fi
}
 



yaim_query_conf_node_gsiftp_door()
{
  # Returns 0 when node is not a gsiftp door node
  # Returns 1 when node is a gsiftp door node
  local result
  local thishost
  local hostspec
  thishost=`hostname -f`
  result=0
  if [ -z "$DCACHE_DOOR_GSIFTP" ] ; then
    yaim_query_conf_node_dcache_pool
    result=$?
  else
    thishost=`hostname -f`
    hostspec=$(echo "$DCACHE_DOOR_GSIFTP" | grep -o "$thishost[^ ]*")
    if [ "x$hostspec" != "x" ]; then
      result=1
    fi
  fi
  return ${result}
}

#Returns a string of all host:port pairs that the configuration 
#will give cluster wide

yaim_query_conf_dcache_gsiftp_door_node_port_list()
{
  local hostlist
  local thishost
  local thisport
  local thishostport
  local allreadyadded
  thishost=`hostname -f`
  if [ -z "$DCACHE_DOOR_GSIFTP" ] ; then
    yaim_query_conf_dcache_pool_node_list
    thishost=$RET
    RET=""    
    for thishost in ${thishost}
    do
      allreadyadded=$(echo ${RET} | grep ${thishost})
      if [ -z "${allreadyadded}" ] ; then
        RET="$thishost:2811 $RET"
      fi
    done
  else
    hostlist=$DCACHE_DOOR_GSIFTP
    RET=""
    for thishostport in ${hostlist}
    do
      thishost=$(echo ${thishostport} | cut -d: -f1 )
      thisport=$(echo ${thishostport} | cut -s -d: -f2 )
      if [ -z "${thisport}" ] ; then
        thisport=2811
      fi
      RET="$thishost:$thisport $RET"
    done
  fi
}

# Gives the local port to run gsiftp upon

yaim_query_conf_node_gsiftp_door_port()
{
    # Returns 0 when node is not a gsidcap node
    # Returns 1 when node is a gsidcap node
    local thishost
    local NodePortList
    thishost=`hostname -f`
    yaim_query_conf_dcache_gsiftp_door_node_port_list
    NodePortList=$RET
    RET=""
    for ahost in ${NodePortList}
    do
        if [ "${thishost}" == "${ahost:0:${#thishost}}" ] ; then 
            RET="${ahost:${#thishost} + 1}"
            break
        fi
    done
    if [ -z "$RET" ] ; then
        RET=2811
    fi
}


yaim_query_conf_node_dcap_door()
{
  # Returns 0 when node is not a dcap door node
  # Returns 1 when node is a dcap door node
  local result
  local thishost
  local hostspec
  result=0
  thishost=`hostname -f`
  if [ -z "$DCACHE_DOOR_DCAP" ] ; then
    yaim_query_conf_node_dcache_pool
    result=$?
  else
    thishost=`hostname -f`
    hostspec=$(echo "$DCACHE_DOOR_DCAP" | grep -o "$thishost[^ ]*")
    if [ "x$hostspec" != "x" ]; then
      result=1
    fi
  fi
  return ${result}
}


yaim_query_conf_dcache_dcap_door_node_port_list()
{
  local hostlist
  local thishost
  local thishost
  local thishostport
  local thisport
  local allreadyadded
  thishost=`hostname -f`
  if [ -z "$DCACHE_DOOR_DCAP" ] ; then
    yaim_query_conf_dcache_pool_node_list
    hostlist=$RET
    RET=""    
    for thishost in ${hostlist}
    do
      allreadyadded=$(echo ${RET} | grep ${thishost})
      if [ -z "${allreadyadded}" ] ; then
        RET="$thishost:22125 $RET" 
      fi
    done
  else
    hostlist=$DCACHE_DOOR_DCAP
    RET=""
    for thishostport in ${hostlist}
    do
      thishost=$(echo ${thishostport} | cut -d: -f1 )
      thisport=$(echo ${thishostport} | cut -s -d: -f2 )
      if [ -z "${thisport}" ] ; then
        thisport=22125
      fi
      RET="$thishost:$thisport $RET"
    done
  fi
}


yaim_query_conf_node_dcap_door_port()
{
    # Returns 0 when node is not a gsidcap node
    # Returns 1 when node is a gsidcap node
    local thishost
    local NodePortList
    thishost=`hostname -f`
    yaim_query_conf_dcache_dcap_door_node_port_list
    NodePortList=$RET
    RET=""
    for ahost in ${NodePortList}
    do
        if [ "${thishost}" == "${ahost:0:${#thishost}}" ] ; then 
            RET="${ahost:${#thishost} + 1}"
            break
        fi
    done
    if [ -z "$RET" ] ; then
        RET=22125
    fi
}

yaim_query_conf_node_gsidcap_door()
{
  # Returns 0 when node is not a gsidcap node
  # Returns 1 when node is a gsidcap node
  local result
  local thishost
  local hostspec
  
  result=0
  if [ -z "$DCACHE_DOOR_GSIDCAP" ] ; then
    yaim_query_conf_node_dcache_pool
    result=$?
  else
    thishost=`hostname -f`
    hostspec=$(echo "$DCACHE_DOOR_GSIDCAP" | grep -o "$thishost[^ ]*")
    if [ "x$hostspec" != "x" ]; then
      result=1
    fi
  fi
  return ${result}
}

yaim_query_conf_dcache_gsidcap_door_node_port_list()
{
  local hostlist
  local thishost
  local thishostport
  local thisport
  local allreadyadded

  if [ -z "$DCACHE_DOOR_GSIDCAP" ] ; then
    yaim_query_conf_dcache_pool_node_list
    hostlist=$RET
    RET=""    
    for thishost in ${hostlist}
    do
      allreadyadded=$(echo ${RET} | grep ${thishost})
      if [ -z "${allreadyadded}" ] ; then
        RET="$thishost:22128 $RET" 
      fi
    done
  else
    hostlist=$DCACHE_DOOR_GSIDCAP
    RET=""
    for thishostport in ${hostlist}
    do
      thishost=$(echo ${thishostport} | cut -d: -f1 )
      thisport=$(echo ${thishostport} | cut -s -d: -f2 )
      if [ -z "${thisport}" ] ; then
        thisport=22128
      fi
      RET="$thishost:$thisport $RET"
    done
  fi
}

yaim_query_conf_node_gsidcap_door_port()
{
    # Returns 0 when node is not a gsidcap node
    # Returns 1 when node is a gsidcap node
    local thishost
    local NodePortList
    thishost=`hostname -f`
    yaim_query_conf_dcache_gsidcap_door_node_port_list
    NodePortList=$RET
    RET=""
    for ahost in ${NodePortList}
    do
        if [ "${thishost}" == "${ahost:0:${#thishost}}" ] ; then 
            RET="${ahost:${#thishost} + 1}"
            break
        fi
    done
    if [ -z "$RET" ] ; then
        RET=22128
    fi
}

yaim_query_conf_node_xrootd_door()
{
  # Returns 0 when node is not a xrootd node
  # Returns 1 when node is a xrootd node
  local result
  local thishost
  local hostspec
  
  result=0
  if [ -n "$DCACHE_DOOR_XROOTD" ] ; then
    thishost=`hostname -f`
    hostspec=$(echo "$DCACHE_DOOR_XROOTD" | grep -o "$thishost[^ ]*")
    if [ "x$hostspec" != "x" ]; then
      result=1
    fi
  fi
  return ${result}
}

yaim_query_conf_dcache_xrootd_door_node_port_list()
{
  local hostlist
  local thishost
  local thishostport
  local thisport
  local allreadyadded

  if [ -z "$DCACHE_DOOR_XROOTD" ] ; then
    yaim_query_conf_dcache_pool_node_list
    hostlist=$RET
    RET=""    
    for thishost in ${hostlist}
    do
      allreadyadded=$(echo ${RET} | grep ${thishost})
      if [ -z "${allreadyadded}" ] ; then
        RET="$thishost:22128 $RET" 
      fi
    done
  else
    hostlist=$DCACHE_DOOR_XROOTD
    RET=""
    for thishostport in ${hostlist}
    do
      thishost=$(echo ${thishostport} | cut -d: -f1 )
      thisport=$(echo ${thishostport} | cut -s -d: -f2 )
      if [ -z "${thisport}" ] ; then
        thisport=1094
      fi
      RET="$thishost:$thisport $RET"
    done
  fi
}

yaim_query_conf_node_xrootd_door_port()
{
    # Returns 0 when node is not a xrootd node
    # Returns 1 when node is a xrootd node
    local thishost
    local NodePortList
    thishost=`hostname -f`
    yaim_query_conf_dcache_xrootd_door_node_port_list
    NodePortList=$RET
    RET=""
    for ahost in ${NodePortList}
    do
        if [ "${thishost}" == "${ahost:0:${#thishost}}" ] ; then 
            RET="${ahost:${#thishost} + 1}"
            break
        fi
    done
    if [ -z "$RET" ] ; then
        RET=1094
    fi
}




yaim_query_conf_node_dcache_door()
{
    # Returns 0 when node is not a door node
    # Returns 1 when node is a door node
    local resultset
    local result
    yaimlog DEBUG "function yaim_query_conf_node_dcache_door start"
    resultset=""
    result=0
    yaim_query_conf_node_srm_door
    resultset=${resultset}$?
    yaim_query_conf_node_gsiftp_door
    resultset=${resultset}$?
    yaim_query_conf_node_dcap_door
    resultset=${resultset}$?
    yaim_query_conf_node_gsidcap_door
    resultset=${resultset}$?
    yaim_query_conf_node_xrootd_door
    resultset=${resultset}$?
    yaim_query_conf_infoprovider_system
    resultset=${resultset}$?
    yaim_query_conf_info_system
    resultset=${resultset}$?
    if [ -n "$(echo $resultset | grep 1)" ] ; then
        result=1
    fi
    yaimlog DEBUG "function yaim_query_conf_node_dcache_door stop"
    return ${result}
}

yaim_query_conf_node_dcache_dbserver()
{
    # Returns 0 when node is not srm_door node
    # Returns 1 when node is srm_door node
    local resultset
    local result
    resultset=""
    result=0
    yaim_query_conf_node_srm_door
    resultset=${resultset}$?
    yaim_query_conf_node_dcache_name_server
    resultset=${resultset}$?
    if [ -n "$(echo $resultset | grep 1)" ] ; then
        result=1
    fi
    return ${result}
}




yaim_query_conf_voms()
{
  yaimlog DEBUG "function yaim_query_conf_voms start"
  result=0
  # Returns 0 if VOMS generation is not available
  # Returns 1 if VOMS generation available
  if [ -x /opt/d-cache/sbin/dcacheVoms2Gplasma.py ] ; then
    result=1
  fi
  yaimlog DEBUG "function yaim_query_conf_voms stop"
  return ${result}
}

yaim_query_conf_kpwd()
{
  local result
  local filetocheck
  result=0
  filetocheck=$1
  # Returns 0 if VOMS generation is not available
  # Returns 1 if VOMS generation available
  if [ -x /opt/edg/sbin/edg-mkgridmap ] ; then
    if [ -x /opt/d-cache/bin/grid-mapfile2dcache-kpwd ] ; then
      result=1
    fi
  fi
  return ${result}
}

yaim_query_conf_modifed_today()
{
  # Returns 0 if not modified today
  # Returns 1 if modified today
  local result
  local filetocheck
  local currentdate
  local seldate
  
  
  result=0
  filetocheck=$1
  
  if [ -f ${filetocheck} ] ; then
    currentdate=$(date +%Y-%m-%d)
    seldate=$(stat ${filetocheck} | grep Modify | cut -d: -f2- | grep "${currentdate}" )
    if [ -z "${seldate}" ] ; then
      result=1
    else
      result=0
    fi
  fi
}



## End of yaim query functions
## Start of yaim state functions

# Takes parameters of which components need restarting
# Recongnised parameters are "core" "pool" "pnfs" "rdbms" "chimera"
yaim_state_service_restart_needed()
{
    local param
    local matcher
    while [ $# -ne 0 ]
    do
        param=$1
        matcher=$(echo ${YAIM_STATE_DCACHE_RESTART} | grep ${param})
        if [ -z "${matcher}" ] ; then
            YAIM_STATE_DCACHE_RESTART="${param} ${YAIM_STATE_DCACHE_RESTART}"
        fi
        shift 1
    done

    RET=$YAIM_STATE_DCACHE_RESTART
}


yaim_state_nameserver_unmount()
{
  local mountedDev
  mountedDev=$(mount |  grep pnfs |  sed -e "s/ .*$//" )
  if [ -n "${mountedDev}" ] ; then
    umount ${mountedDev}
  fi
}

yaim_state_nameserver_stop()
{
  yaim_query_conf_node_dcache_name_type
  nameserverType=$RET
  if [ "${nameserverType}" = "pnfs" ] ; then
    config_pnfs_stop
  fi
  if [ "${nameserverType}" = "chimera" ] ; then
    chimera_create_nfs_stop
  fi
}


yaim_state_service_restart_needed_do()
{
    local has_pnfsserver
    local has_rdbms
    local has_pnfs
    local has_door
    local has_admin
    local has_pool
    local shouldruncore
    local rdbms_restart
    local pnfs_restart
    local chimera_restart
    local core_restart
    local pool_restart
    local ysdrn
    rdbms_restart=0
    pnfs_restart=0
    chimera_restart=0
    core_restart=0
    pool_restart=0
    
    # Added postgresql in order to solve problems with
    # postmaster not listening after initial install
    yaim_query_conf_node_dcache_pnfsserver
    has_pnfsserver=$?
    yaim_query_conf_node_dcache_dbserver
    has_rdbms=$?
    yaim_query_conf_node_dcache_nameserver_user
    has_pnfs=$?
    yaim_query_conf_node_dcache_chimera
    has_chimera=$?
    yaim_query_conf_node_dcache_door
    has_door=$?
    yaim_query_conf_node_dcache_admin
    has_admin=$?
    yaim_query_conf_node_dcache_pool
    has_pool=$?
    for service in $YAIM_STATE_DCACHE_RESTART
    do
        case "${service}" in
        
        "rdbms" )
            rdbms_restart=1
            pnfs_restart=1
            core_restart=1
            pool_restart=1
        ;;
        "pnfs" )
            pnfs_restart=1
            core_restart=1
            pool_restart=1
        ;;
        "chimera" )
            chimera_restart=1
            core_restart=1
            pool_restart=1
        ;;
        "core" )
            core_restart=1
            pool_restart=1
        ;;
        "pool" )
            pool_restart=1
            core_restart=1
        ;;
        * )
            ysdrn="${service} ${ysdrn}"
        ;;
        esac
    done
    shouldruncore=${has_door}${has_admin}${has_pnfsserver}${has_chimera}
    if [ -n "$(echo "${shouldruncore}${has_pool}" | grep 1)" -a "${core_restart}" == "1" ] ; then
        /sbin/service dcache stop
    fi

    if [ "$has_pnfs" == "1" -a "${pnfs_restart}" == "1" ] ; then
        config_pnfs_stop
    fi

    if [ "$has_chimera" == "1" -a "${chimera_restart}" == "1" ] ; then
        chimera_create_nfs_stop
    fi

    if [ "$has_rdbms" == "1" -a "${rdbms_restart}" == "1" ] ; then
        /sbin/service postgresql stop
        /sbin/service postgresql start
    fi

    if [ "$has_pnfs" == "1" -a "${pnfs_restart}" == "1" ] ; then
        config_pnfs_start
    fi
    
    if [ "$has_chimera" == "1" -a "${chimera_restart}" == "1" ] ; then
        chimera_create_nfs_start
    fi
    if [ -n "$(echo "${shouldruncore}${has_pool}" | grep 1)" -a "${core_restart}" == "1" ] ; then
        /sbin/service dcache start
    fi

    YAIM_STATE_DCACHE_RESTART=${ysdrn}
}



yaim_state_java_path()
{
    local javaconf
    local javaloc
    RET=""
    if [ -n "$JAVA_LOCATION" ] ; then
      yaimlog INFO "Setting \$JAVA_LOCATION in site-info.def is depricated."
      yaimlog INFO "The Java path is now configured through /etc/java/java.conf"
      
      javaloc=${JAVA_LOCATION}/bin/java
      if [ -x "${javaloc}" ] ; then
        RET=$javaloc
      else
        yaimlog ERROR "Could not find Java at $javaloc"
        exit 1
      fi
    fi
    if [ -z "$RET" ] ; then

      javaconf=/etc/java/java.conf
      test -f $javaconf && . $javaconf

      RET=$JAVA_HOME/bin/java
      test -x "$RET" || RET=

      [ -z "$RET" ] && RET=`ls /usr/java/j2re*/bin/java  2> /dev/null | tail -n 1`
      [ -z "$RET" ] && RET=`ls /usr/java/j2sdk*/bin/java 2> /dev/null | tail -n 1`
      [ -z "$RET" ] && RET=`ls /usr/java/jdk*/bin/java 2> /dev/null | tail -n 1`
      [ -z "$RET" ] && return 1
    fi
    return 0
}


yaim_state_reset_dcache_config() 
{
  # Returns 0 when config is fine
  # Returns 1 when config should be reset
  local result
  
  result=0  
  if test "X$RESET_DCACHE_CONFIGURATION" != Xyes
  then
    if [ ! -f /opt/d-cache/etc/node_config ] ; then
        RESET_DCACHE_CONFIGURATION=yes
    fi
  fi
  if [ "X$RESET_DCACHE_CONFIGURATION" == Xyes ]
  then
    result=1
  fi
  return $result
}



## End of yaim state functions
## Start of yaim config functions



check_portmap()
{
  local PPORT
  PPORT=111

  if test `netstat -tapn | grep portmap | grep :${PPORT} | wc -l` -eq 0 ; then
    return 1
  fi
  
  return 0
}

config_sedcache_check_sanity () 
{
    yaimlog DEBUG "function config_sedcache_check_sanity start"
    # Start portmapper
    check_portmap || {
        echo "Portmapper is not running, trying to start portmap service." >&2
        /sbin/service portmap start
    }
    check_portmap || {
        echo "Portmapper is not running, please start it before running this script." >&2
        return $?
    }
    yaimlog DEBUG "function config_sedcache_check_sanity stop"
}



config_sedcache_start() 
{
    local rcpnfsserver
    local rcrdbms
    local rcpnfs
    local rcdoor
    local rcadmin
    local rcpool
    local shouldruncore
    
    # Added postgresql in order to solve problems with
    # postmaster not listening after initial install
    yaim_query_conf_node_dcache_pnfsserver
    rcpnfsserver=$?
    yaim_query_conf_node_dcache_dbserver
    rcrdbms=$?
    yaim_query_conf_node_dcache_nameserver_user
    rcpnfs=$?
    yaim_query_conf_node_dcache_door
    rcdoor=$?
    yaim_query_conf_node_dcache_admin
    rcadmin=$?
    yaim_query_conf_node_dcache_pool
    rcpool=$?

    if [ "$rcpool" == "1" ]
    then
        /sbin/service dcache stop
    fi

    shouldruncore=${rcdoor}${rcadmin}${rcpnfsserver}
    if [ -n "$(echo $shouldruncore | grep 1)" ] ; then
        /sbin/service dcache stop
    fi

    if [ "$rcpnfs" == "1" ] ; then
        config_pnfs_stop
        chimera_create_nfs_stop
    fi

    if [ "$rcrdbms" == "1" ] ; then
        /sbin/service postgresql stop
        /sbin/service postgresql start
    fi


    if [ "$rcpnfs" == "1" ] ; then
        config_pnfs_start
        chimera_create_nfs_start
    fi

    if [ -n "$(echo $shouldruncore | grep 1)" ] ; then
        /sbin/service dcache start
    fi

    if [ "$rcpool" == "1" ]
    then
        /sbin/service dcache start
    fi
}



config_dcache_config_file_generate_dCacheSetup() 
{
  yaimlog DEBUG "function config_dcache_config_file_generate_dCacheSetup start"
  local SOURCE
  local DESTINATION

  SOURCE=$1
  DESTINATION=$2

  yaim_config_file_update_prepare $DESTINATION
  file2update=$RET

  cp $SOURCE $file2update
  yaim_config_file_update_done $DESTINATION
  if [ $? == 1 ] ; then 
    yaimlog WARNING "Changed the file ${DESTINATION}"
    yaim_state_service_restart_needed core
  fi
  yaimlog DEBUG "function config_dcache_config_file_generate_dCacheSetup stop"
}

config_dcache_config_file_generate_node_config() 
{
  local SOURCE
  local DESTINATION
  local tmptrue
  local file2update
  SOURCE=$1
  DESTINATION=$2
  yaim_config_file_update_prepare $DESTINATION
  file2update=$RET
  cp $SOURCE $file2update
  yaim_config_file_update_done $DESTINATION
  if [ $? == 1 ] ; then 
      yaimlog WARNING "Changed the file ${DESTINATION}"
      yaim_state_service_restart_needed core
  fi
}


config_dcache_config_pools() 
{
  yaimlog DEBUG "function config_dcache_config_pools start"
  #local file2update
  #yaim_config_file_update_prepare ${DESTINATION}
  #file2update=$RET
  #rm -f ${file2update}
  #touch ${file2update}
  yaim_query_conf_dcache_pool_node_size_path
  localpools=$RET
  #echo YYYYYYYYYYYYYYYYYYYY $localpools
  let alreadyexistingpools=`/opt/d-cache/bin/dcache pool ls | wc -l`
  let alreadyexistingpools="$alreadyexistingpools-3"
  for poolspec in `echo $localpools`
  do
    yaimlog DEBUG "processing pool=$poolspec"
    pool=${poolspec##*:}
    size=${poolspec%%:*}
    #echo pool=$pool
    #echo size=$size
    if [ "${pool:0:1}" != "/" ]; then
        echo "[ERROR]: pool path must be absolute:"
        echo "[ERROR]: $poolspec"
        exit 1
    fi
    if [ "$pool" == "$size" ] ; then
        size=0
    fi
    if [ ! -d "${pool}" ] ; then
      mkdir -p $pool
      if test "x$size" = x || test $size -eq 0 ; then  
          size=`df -k $pool | awk 'NR == 2 { print int($4 / 1024 / 1024) }'`
      fi
      rmdir $pool
      if [ $size -lt 4 ] ; then
        yaimlog WARNING "pool size must be larger than 4Gb"
      else
        poolExists=$( /opt/d-cache/bin/dcache pool ls | grep ${pool} )
        if [ -z "$poolExists" ] ; then
          /opt/d-cache/bin/dcache pool create ${size}G ${pool}
          poolcreaterc=$?
          if [ "$poolcreaterc" == "0" ] ; then
            notnamed="0"
            while [ ${notnamed} == "0" ]
            do
              poolname=`hostname`_${alreadyexistingpools}
              poolExists=$( /opt/d-cache/bin/dcache pool ls | grep ${poolname} )
              if [ -z "$poolExists" ] ; then
                /opt/d-cache/bin/dcache pool add ${poolname} ${pool}
                namedPoolrc=$?
                if [ "${namedPoolrc}" == "0" ] ; then
                  yaim_state_service_restart_needed pool
                  notnamed="1"
                fi
              else
                yaimlog INFO "Pool name ${poolname} already exists creating new pool name."
              fi
              # Now we add 1 to the number of the next pool cell name
              let alreadyexistingpools="$alreadyexistingpools+1"
            done
          fi
        else
          yaimlog ERROR "${poolExists}"
        fi
      fi
    fi
  done
  yaimlog DEBUG "function config_dcache_config_pools stop"
}



config_dcache_config_file() 
{
    yaimlog DEBUG "function config_dcache_config_file start"
    local FILETOPROCESS
    local DIRECTORY
    local DESTFILE
    local pairentdir
    local uniqueId
    local GENERATE
    
    FILETOPROCESS=$1
    DIRECTORY=$(dirname $FILETOPROCESS)
    #echo DIRECTORY=$DIRECTORY
    #Note "uniqueId" is made up of file name and pairent dir as files are not unique enough

    #uniqueId
    DESTFILE=$(basename $FILETOPROCESS | sed -e "s/\.temp.*$//")

    pairentdir=$(basename  ${DIRECTORY} )
    uniqueId="${pairentdir}/${DESTFILE}"
    if [ ! -f ${FILETOPROCESS} ]
    then
        GENERATE=yes
    fi

    if test "X$RESET_DCACHE_CONFIGURATION" = Xyes
    then
        GENERATE=yes
    fi

    if [ "X${GENERATE}" == Xyes ]
    then
        yaimlog DEBUG "Generating ${DIRECTORY}/${DESTFILE}"
        case "${uniqueId}" in
        "config/dCacheSetup")
            config_dcache_config_file_generate_dCacheSetup "${FILETOPROCESS}" "${DIRECTORY}/${DESTFILE}"
        ;;
        "etc/dCacheSetup")
            # See function config_dcache_config_template_files
            # for details!
            
            yaimlog DEBUG "To avoid confusion skipping '${FILETOPROCESS}'"
        ;;
        "config/defaultPools.poollist.temp")
            yaimlog DEBUG "Skipping config/defaultPools.poollist"
        ;;
        "etc/node_config")
            config_dcache_config_file_generate_node_config "${FILETOPROCESS}" "${DIRECTORY}/${DESTFILE}"
        ;;
        "etc/dcache.kpwd")
            yaimlog DEBUG "Generated by cron so skipping '${FILETOPROCESS}'"
        ;;
            * )
            #cp ${FILETOPROCESS} ${DIRECTORY}/${DESTFILE}
            yaimlog DEBUG "Skipping ${FILETOPROCESS}"
        ;;
        esac
    fi
    yaimlog DEBUG "function config_dcache_config_file stop"
}



config_dcache_config_template_files()
{
    #This is a nasty hack to fix a document bug in D-Cache 1.6.6-*
    # to quote form the D-Cache book.

    # The central configuration file of a dCache instance is 
    # /opt/d-cache/config/dCacheSetup. For most installation 
    # it is only necessary to set the variable java to the 
    # binary of the java VM and the variable serviceLocatorHost 
    # to the hostname of the admin node. Note that the file has 
    # to go into the subdirectory config/  even though the 
    # template is found in etc/.

    local etctmpfile
    local configtmpfile
    local etctmpfilecs
    local configtmpfilecs

    etctmpfile=${INSTALL_ROOT}/d-cache/etc/dCacheSetup.template
    configtmpfile=${INSTALL_ROOT}/d-cache/config/dCacheSetup.template
    if [ ! -f configtmpfile ] ; then
        cp ${etctmpfile} ${configtmpfile}
    fi

    etctmpfilecs=$(md5sum $etctmpfile)
    configtmpfilecs=$(md5sum $configtmpfile)
    if [ "$etctmpfilecs" != "$configtmpfilecs" ] ;  then
        cp ${etctmpfile} ${configtmpfile}
    fi
}



config_dcache_config_files() 
{
    yaimlog DEBUG "function config_dcache_config_files start"
    local  tmptrue
    local dir
    yaim_state_reset_dcache_config
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        dir=${INSTALL_ROOT}/d-cache
        config_dcache_config_template_files
        FILES_TO_PROCESS=`ls ${INSTALL_ROOT}/d-cache/etc/*.template ${INSTALL_ROOT}/d-cache/config/*template`
        for FILE in $FILES_TO_PROCESS
        do
            config_dcache_config_file $FILE
        done


    else
        echo Skipping D-Cache configuration file reset
    fi
    yaimlog DEBUG "function config_dcache_config_files stop"
}

config_dcache_config_system_files_init() 
{
  yaimlog DEBUG "function config_dcache_config_system_files_init start"
  local dir
  local pnfs
  local rcpnfs
  local rcdoor
  local rcadmin
  local rcpool
  local runcore
  local install
  local rcchimera
  dir=${INSTALL_ROOT}/d-cache
  yaim_query_conf_node_dcache_pnfs_root
  pnfs=${RET}
  yaim_query_conf_node_dcache_chimera
  rcchimera=$?
  yaim_query_conf_node_dcache_pnfsserver
  rcpnfs=$?
  yaim_query_conf_node_dcache_door
  rcdoor=$?
  yaim_query_conf_node_dcache_admin
  rcadmin=$?
  yaim_query_conf_node_dcache_pool
  rcpool=$?
  if [ "$rcpnfs" == "1" ]
  then
    config_sepnfs_installed
    ln -fs $pnfs/bin/pnfs       /etc/init.d
    chkconfig pnfs on
  fi
  if [ "$rcchimera" == "1" ]
  then
    chimera_create_link
  fi
  runcore=${rcdoor}${rcadmin}${rcpnfs}${rcchimera}
  if [ -n "$(echo "$runcore$rcpool" | grep 1)" ] ; then
    ln -fs $dir/bin/dcache /etc/init.d
    chkconfig dcache on
  fi
  yaimlog DEBUG "function config_dcache_config_system_files_init stop"
}



config_dcache_config_system_files() 
{
  yaimlog DEBUG "function config_dcache_config_system_files start"
  #config_dcache_config_system_files_fstab
  yaimlog DEBUG "function config_dcache_config_system_files stop"
}

config_dcache_config_dcache_scripts() 
{
    yaimlog DEBUG "function config_dcache_config_dcache_scripts start"
    local dir
    local retcode
    dir=${INSTALL_ROOT}/d-cache
    $dir/install/install.sh
    retcode=$?
    if [ "$retcode" != "0" ] ; then 
        echo "ERROR executing $dir/install/install.sh failed (exit code $retcode) -- abort"
        exit 1
    fi
    yaimlog DEBUG "function config_dcache_config_dcache_scripts stop"
}



config_sedcache_cron_add_voms() 
{
local minute
local h1
local h2
local h3
local h4
local jobtime
local job
let minute="$RANDOM%60"
let h1="$RANDOM%6"
let h2="$h1+6"
let h3="$h1+12"
let h4="$h1+18"
query=$1
jobtime="$minute $h1,$h2,$h3,$h4 * * * "
job="${INSTALL_ROOT}/d-cache/sbin/dcacheVoms2Gplasma.py \
-r -a -c ${INSTALL_ROOT}/d-cache/etc/dcacheVoms2Gplasma.conf"
if [ "${query}" == "query" ] ; then
  job="${job} -q"
fi
# Now remove cron job
rm -f ${CRON_DIR:-/etc/cron.d}/dcacheVoms2Gplasma
# Now replace cron job
${job}
cron_job dcacheVoms2Gplasma root "${jobtime}${job}"

}

config_sedcache_cron_add_kpwd() 
{
local minute
local h1
local h2
local h3
local h4
local jobtime
local job
let minute="$RANDOM%60"
let h1="$RANDOM%6"
let h2="$h1+6"
let h3="$h1+12"
let h4="$h1+18"
jobtime="$minute $h1,$h2,$h3,$h4 * * * "
job="${INSTALL_ROOT}/edg/sbin/edg-mkgridmap \
--output=/etc/grid-security/grid-mapfile --safe ; \
${INSTALL_ROOT}/d-cache/bin/grid-mapfile2dcache-kpwd"
# Now remove cron job
rm -f ${CRON_DIR:-/etc/cron.d}/edg-mkgridmap
${job}
# Now replace cron job
cron_job edg-mkgridmap root "${jobtime}${job}"

}


config_sedcache_cron() 
{
yaimlog DEBUG "function config_sedcache_cron start"
local minute
local h1
local h2
local h3
local h4
local job
local outfile
local configfile
local rgma_user
local rcNameServer

local file2update
local todaysDate
local thisVo
local pnfsbasedir

let minute="$RANDOM%60"

let h1="$RANDOM%6"
let h2="$h1+6"
let h3="$h1+12"
let h4="$h1+18"



yaim_query_conf_voms
usevoms=$?
yaim_query_conf_kpwd
usekpwd=$?

if [ "${usekpwd}${usevoms}" == "11" ] ; then
  config_sedcache_cron_add_kpwd
  config_sedcache_cron_add_voms
fi

if [ "${usekpwd}${usevoms}" == "10" ] ; then
  config_sedcache_cron_add_kpwd
fi

if [ "${usekpwd}${usevoms}" == "01" ] ; then
  config_sedcache_cron_add_voms query
fi

if [ "${usekpwd}${usevoms}" == "00" ] ; then
  yaimlog WARNING "both kpwd and voms are not set up by YAIM"
fi


## Information Provider

# Creating script for dynamic info generation


outfile=$INSTALL_ROOT/lcg/var/gip/plugin/lcg-info-dynamic-se
configfile=$INSTALL_ROOT/lcg/var/gip/ldif/static-file-SE.ldif

if [ -f $configfile ]; then
   yaimlog ABORT "$configfile does not exist"


   cat <<EOF>$outfile
#!/bin/sh
#
# Script dynamically generated by YAIM function ${FUNCNAME[0]}
#

${INSTALL_ROOT}/lcg/libexec/lcg-info-dynamic-dcache $configfile

EOF

   chmod +x $outfile
fi


# Crontab entry to have up-to-date proxy for dynamic info query
if [ -x ${INSTALL_ROOT}/globus/bin/grid-proxy-init ] ; then
  job="7 * * * * ${INSTALL_ROOT}/globus/bin/grid-proxy-init -q \
-cert /etc/grid-security/hostcert.pem \
-key /etc/grid-security/hostkey.pem -out ${INSTALL_ROOT}/lcg/hostproxy.\$$ && \
chown edginfo ${INSTALL_ROOT}/lcg/hostproxy.\$$ && \
mv -f ${INSTALL_ROOT}/lcg/hostproxy.\$$ ${INSTALL_ROOT}/lcg/hostproxy"

cron_job edginfo-proxy root "$job"

echo "$job" | sed 's|[^/]*||' | sh

rgma_user=rgma

job="8 * * * * ${INSTALL_ROOT}/globus/bin/grid-proxy-init -q \
-cert /etc/grid-security/hostcert.pem \
-key /etc/grid-security/hostkey.pem -out ${INSTALL_ROOT}/lcg/hostproxy.\$$ && \
chown $rgma_user ${INSTALL_ROOT}/lcg/hostproxy.\$$ && \
mv -f ${INSTALL_ROOT}/lcg/hostproxy.\$$ ${INSTALL_ROOT}/lcg/hostproxy.$rgma_user"

cron_job rgma-proxy root "$job"

fi
echo "$job" | sed 's|[^/]*||' | sh

config_sepnfs_cron

yaimlog DEBUG "function config_sedcache_cron stop"
}



config_dcache_kpwd_update()
{
yaimlog DEBUG "function config_dcache_kpwd_update start"
local rcdoor
local currentdate
local seldate
local fileneeds_update
local gridmapfileloc
local gridmapfile2dcache
gridmapfile2dcache=/opt/d-cache/bin/grid-mapfile2dcache-kpwd
gridmapfileloc=/etc/grid-security/grid-mapfile
if [ -f ${gridmapfileloc} ] ; then
    fileneeds_update=0
    currentdate=$(date +%Y-%m-%d)
    seldate=$(stat ${gridmapfileloc} | grep Modify | cut -d: -f2- | grep "${currentdate}" )
    if [ -z "${seldate}" ] ; then
        fileneeds_update=1
    fi
else
    fileneeds_update=1
fi
if [ "$fileneeds_update" == "1" ] ; then
  if [ -x /opt/edg/sbin/edg-mkgridmap ] ; then
    /opt/edg/sbin/edg-mkgridmap --output=${gridmapfileloc} --safe
  fi
fi
if [ -f ${gridmapfile2dcache} ] ; then
  ${gridmapfile2dcache}
fi
yaimlog DEBUG "function config_dcache_kpwd_update stop"
}


config_dcache_gen_voms2gplasmaconf_header()
{
  file2update=$1
  if [ "X${file2update}" == "X" ] ; then
    yaimlog ERROR "config_dcache_gen_voms2gplasmaconf_header called with no parameter"
    exit 1
  fi
  echo "# grid-vorolemap path" >> ${file2update}
  echo "vorolemap /etc/grid-security/grid-vorolemap" >> ${file2update}
  echo >> ${file2update}
  echo "# authzdb path" >> ${file2update}
  echo "authzdb /etc/grid-security/storage-authzdb" >> ${file2update}
  echo >> ${file2update}
}


config_dcache_gen_voms2gplasmaconf()
{
yaimlog DEBUG "function config_dcache_gen_voms2gplasmaconf start"
local dcacheVoms2GplasmaConf
local confexists
local file2update
dcacheVoms2GplasmaConf=$1
confexists=0
if [ -f "${dcacheVoms2GplasmaConf}" ] ; then
  confexists=1
  yaim_config_file_update_prepare ${dcacheVoms2GplasmaConf}
  file2update=$RET
  yaim_state_reset_dcache_config
  rcupdateconfig=$?
  if [ "$rcupdateconfig" == "1" ]; then
    rm -f ${file2update}
    config_dcache_gen_voms2gplasmaconf_header ${file2update}
  fi
else
  file2update=${dcacheVoms2GplasmaConf}
  config_dcache_gen_voms2gplasmaconf_header ${file2update}
fi

for thisvo in $(echo $VOS)
do
  voupper=$(echo $thisvo | tr '[:lower:].\-' '[:upper:]__')
  thisvoserversel="VO_${voupper}_VOMS_SERVERS"
  #echo thisvoserversel=$thisvoserversel
  thisvoserverlist=$(eval echo '$'`echo $thisvoserversel`)
  # Now we must iterate through each of the servers.
  split_quoted_variable $thisvoserverlist | while read thisvoserver; do
    #echo thisvoserver=$thisvoserver
    #vomss://voms.cern.ch:8443/voms/lhcb?/lhcb/ -> 
    #https://voms.cern.ch:8443/voms/ops/services/VOMSCompatibility?method=getGridmapUsers&container=/ops
    transvoserver=$(echo $thisvoserver | sed -e "s/\/$//" | sed -e "s/vomss/https/" | sed -e "s/\?/\/services\/VOMSCompatibility\?method=getGridmapUsers\&container=/" )

    users_VoFlagGetsNames ${thisvo} 
    VoFlagGetsNamesRC=$?
    username=`echo "$RET" | head -1`
    if [ "${VoFlagGetsNamesRC}" != "0" ] ; then
      yaimlog ERROR "VoFlagGetsNames ${thisvo} returned an error exiting."
      exit 1
    fi
    if [ -z "${username}" ] ; then
      yaimlog WARNING "VoFlagGetsNames vo="${thisvo}" returned no users so not supporting this VO."
      yaimlog WARNING "Please check the \$USERS_CONF in site-info.def and that the file exists and contains an entry for ${thisvo} with no flags."
      continue
    fi
    users_NameGetsUid ${username}
    useridexists=$?
    userid="$RET"
    users_NameGetsGid ${username}
    groupIdExistsRc=$?
    if [ "${groupIdExistsRc}" != "0" ] ; then
      yaimlog ERROR "users_NameGetsGid ${thisvo} returned an error exiting."
      exit 1
    fi
    groupid=$(echo "$RET" | cut -d, -f1 )
    if [ "0" == "${useridexists}" ] ; then
      allreadyhere=$(grep "$transvoserver" ${file2update})
      if [ -z "${allreadyhere}" ] ; then
        echo >> ${file2update}
        echo "# Mapping ${thisvo} members to ${username} uid=${userid} gid=${groupid}" >> ${file2update}
        echo "group ${transvoserver} ${username} ${userid} ${groupid}" >> ${file2update}
        echo >> ${file2update}
      fi
    fi
    
    
    users_VoGetsFlags ${thisvo}
    VoGetsFlagsRc=$?
    VoFlags=$RET
    if [ "${VoGetsFlagsRc}" != "0" ] ; then
      yaimlog ERROR "users_VoGetsFlags ${thisvo} had an error skipping processing the rest of the VO."
      exit 1
    fi
    
    for VoFlag in ${VoFlags}
    do
      mappedValue=""
      if [ "${VoFlag}" == "sgm" ] ; then
        mappedValue="/Role=lcgadmin"
      fi
      if [ "${VoFlag}" == "prd" ] ; then
        mappedValue="/Role=production"
      fi
      
      users_VoFlagGetsNames ${thisvo} ${VoFlag}
      VoFlagGetsNamesRC=$?
      username=`echo "$RET" | head -1`
      if [ "${VoFlagGetsNamesRC}" != "0" ] ; then
        yaimlog ERROR "VoFlagGetsNames vo=${thisvo} role=${mappedValue} retrurned an error exiting."
        exit 1
      fi
      if [ -z "${username}" ] ; then
        yaimlog WARNING "VoFlagGetsNames vo=${thisvo} role=${mappedValue} returned no users so not supporting this VO role."
        continue
      fi
      users_NameGetsUid ${username}
      useridexists=$?
      
      userid="$RET"
      users_NameGetsGid ${username}
      groupIdExistsRc=$?
      if [ "${groupIdExistsRc}" != "0" ] ; then
        yaimlog ERROR "users_NameGetsGid ${thisvo} returned an error exiting."
        exit 1
      fi
      groupid=$(echo "$RET" | cut -d, -f1 )
      if [ "0" == "${useridexists}" ] ; then
        allreadyhere=$(grep "${transvoserver}${mappedValue}" ${file2update})
        if [ -z "${allreadyhere}" ] ; then
          echo >> ${file2update}
          echo "# Mapping ${thisvo}${mappedValue} members to ${username} uid=${userid} gid=${groupid}" >> ${file2update}
          echo "group ${transvoserver}${mappedValue} ${username} ${userid} ${groupid}" >> ${file2update}
          echo >> ${file2update}
        fi
      else
        yaimlog ERROR "users_NameGetsUid could not find UID for ${username}."
        exit 1
      fi 
    done  
    
  done
done


if [ "${confexists}" == "1" ] ; then
  yaim_config_file_update_done ${dcacheVoms2GplasmaConf}
  if [ $? == 1 ] ; then 
    yaimlog WARNING "Changed the file ${dcacheVoms2GplasmaConf}"
  fi
fi

yaimlog DEBUG "function config_dcache_gen_voms2gplasmaconf stop"
}


config_dcache_vorolemap_authz_update()
{
yaimlog DEBUG "function config_dcache_vorolemap_authz_update start"
local rcupdatedauthz
local rcupdatedvorolemap
local rcupdateconfig
local dcacheVoms2GplasmaConf
local authzfile
local vorolemap

authzfile=/etc/grid-security/storage-authzdb
vorolemap=/etc/grid-security/grid-vorolemap
dcacheVoms2GplasmaConf=/opt/d-cache/etc/dcacheVoms2Gplasma.conf

yaim_query_conf_modifed_today ${authzfile}
rcupdatedauthz=$?
yaim_query_conf_modifed_today ${vorolemap}
rcupdatedvorolemap=$?

# check config for dcacheVoms2Gplasma
if [ -f "${dcacheVoms2GplasmaConf}" ] ; then 
  # check config for dcacheVoms2Gplasma exists
  yaim_state_reset_dcache_config
  rcupdateconfig=$?
  if [ "$rcupdateconfig" == "1" ]; then
    # update config for dcacheVoms2Gplasma
    config_dcache_gen_voms2gplasmaconf ${dcacheVoms2GplasmaConf}
  fi
else
  # create config for dcacheVoms2Gplasma
  config_dcache_gen_voms2gplasmaconf ${dcacheVoms2GplasmaConf}
fi

if [ "${rcupdatedauthz}" == "1" ] ;then
  # generate authz
  /opt/d-cache/sbin/dcacheVoms2Gplasma.py -a -c ${dcacheVoms2GplasmaConf}
fi


if [ "${rcupdatedvorolemap}" == "1" ] ;then
  # generate 
  /opt/d-cache/sbin/dcacheVoms2Gplasma.py -rq -c ${dcacheVoms2GplasmaConf}
fi

yaimlog DEBUG "function config_dcache_vorolemap_authz_update stop"
}



yaim_config_pool_manager_add_lines()
{
yaimlog DEBUG "function yaim_config_pool_manager_add_lines start"
local file_location_poolmanager
local file2update
local vodone
local linksearch
local linkdone

file_location_poolmanager=/opt/d-cache/config/PoolManager.conf
yaim_config_file_update_prepare ${file_location_poolmanager}
file2update=$RET
for thiscurrentvo in ${VOS}
do
    vosearch="# This section is for ${thiscurrentvo}"
    vodone=$(grep "${vosearch}" ${file2update})
    if [ -z "${vodone}" ] ; then
        echo "#" >> ${file2update}
        echo "${vosearch}" >> ${file2update}
        echo "#" >> ${file2update}
        echo psu create pgroup ${thiscurrentvo} >> ${file2update}
    echo >> ${file2update}
    fi
    linksearch="# This section is for ${thiscurrentvo} link"
    linkdone=$(grep "${linksearch}" ${file2update})
    if [ -z "${linkdone}" ] ; then
        echo "" >> ${file2update}
        echo "${linksearch}" >> ${file2update}
        echo "" >> ${file2update}
        echo "psu create unit -store ${thiscurrentvo}:STATIC@osm" >> ${file2update}
        echo "psu create unit -store ${thiscurrentvo}:GENERATED@osm" >> ${file2update}
        echo "" >> ${file2update}
        echo "psu addto ugroup any-store ${thiscurrentvo}:STATIC@osm" >> ${file2update}
        echo "psu addto ugroup any-store ${thiscurrentvo}:GENERATED@osm" >> ${file2update}

        echo "" >> ${file2update}
        echo "psu create ugroup ${thiscurrentvo}-groups " >> ${file2update}
        echo "psu addto ugroup ${thiscurrentvo}-groups ${thiscurrentvo}:STATIC@osm" >> ${file2update}
        echo "psu addto ugroup ${thiscurrentvo}-groups ${thiscurrentvo}:GENERATED@osm" >> ${file2update}
        echo >> ${file2update}
        echo "psu create link ${thiscurrentvo}-link world-net ${thiscurrentvo}-groups" >> ${file2update}
        echo "psu set link ${thiscurrentvo}-link -readpref=20 -writepref=20 -cachepref=20" >> ${file2update}
        echo "psu add link ${thiscurrentvo}-link ${thiscurrentvo}" >> ${file2update}
        echo >> ${file2update}
    fi
done
yaim_config_file_update_done ${file_location_poolmanager}
if [ $? == 1 ] ; then 
    yaim_state_service_restart_needed core
fi
yaimlog DEBUG "function yaim_config_pool_manager_add_lines stop"
}


yaim_config_dcache_files_key_value_dCacheSetup()
{
    yaimlog DEBUG "function yaim_config_dcache_files_key_value_dCacheSetup start"
    
    local port_range
    local tmptrue
    local port_range2
    local XROOTDPORT
    local configFile
    local port_range
    local port_range2
    local errorcode
    local portrangeGlobusMin
    local portrangeGlobusMax
    local portrangeNativeMin
    local portrangeNativeMax
    local portrangeClientMin
    local portrangeClientMax
    local SED
    local reusevoms
    if [ "x${1}" == "x" ] ; then
        configFile="/opt/d-cache/config/dCacheSetup"
    else
        configFile=$1
    fi 
    
    yaim_config_file_update_prepare ${configFile}
    file2update=$RET
    
    yaim_query_conf_dcache_portrange_gsiftp_server
    errorcode=$?
    portrangeGlobusMin=$(echo $RET | cut -s -d, -f1)
    portrangeGlobusMax=$(echo $RET | cut -s -d, -f2)
    if [ "$errorcode" != "0" ] ; then
        echo "Error: DCACHE_PORT_RANGE_PROTOCOLS_SERVER_GSIFTP set incorrect format:"
        exit 1
    fi
    #echo portrangeGlobusMin=${portrangeGlobusMin}
    #echo portrangeGlobusMax=${portrangeGlobusMax}
    
    
    yaim_query_conf_dcache_portrange_protocols_native
    errorcode=$?
    portrangeNativeMin=$(echo $RET | cut -s -d, -f1)
    portrangeNativeMax=$(echo $RET | cut -s -d, -f2)
    if [ "$errorcode" != "0" ] ; then
        echo "Error: DCACHE_PORT_RANGE_PROTOCOLS_SERVER_MISC set incorrect format:"
        exit 1
    fi
    #echo portrangeNativeMin=${portrangeNativeMin}
    #echo portrangeNativeMax=${portrangeNativeMax}
    
    
    yaim_config_file_get_value  ${file2update}  java_options  cork.desy.de
    javaOptions=$RET
    SED="s/Dorg\.globus\.tcp\.port\.range=[0-9]*,[0-9]*/Dorg\.globus\.tcp\.port\.range=$portrangeGlobusMin,$portrangeGlobusMax/"
    javaOptions=$(echo ${javaOptions} | sed -e $SED )
    SED="s/Dorg\.dcache\.net\.tcp\.portrange=[0-9]*:[0-9]*/Dorg\.dcache\.net\.tcp\.portrange=$portrangeNativeMin:$portrangeNativeMax/"
    javaOptions=$(echo ${javaOptions} | sed -e $SED )
    yaim_config_file_set_value "${file2update}" "java_options" "\"${javaOptions}\""
    
    yaim_query_conf_dcache_portrange_gsiftp_client
    if [ "$errorcode" != "0" ] ; then
        echo "Error: DCACHE_PORT_RANGE_PROTOCOLS_SERVER_MISC set incorrect format:"
        exit 1
    fi
    portrangeClientMin=$(echo $RET | cut -s -d, -f1)
    portrangeClientMax=$(echo $RET | cut -s -d, -f2)
    yaim_config_file_set_value "${file2update}" "clientDataPortRange" "${portrangeClientMin}:${portrangeClientMax}"    

    yaim_config_file_set_value "${file2update}" "serviceLocatorHost" "$DCACHE_ADMIN"
    
    #yaim_config_file_set_value "${file2update}" "cacheInfo" "companion"    

    yaim_state_java_path
    java_bin=$RET
    yaim_config_file_set_value "${file2update}" "java" "$java_bin"   

    yaim_query_conf_node_gsiftp_door    
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        yaim_query_conf_node_gsiftp_door_port
        GSIFTPPORT=$RET
        yaim_config_file_set_value "${file2update}" "gsiFtpPortNumber" "$GSIFTPPORT"
    fi

    yaim_query_conf_node_srm_door
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        yaim_query_conf_node_srm_door_port
        SRMPORT=$RET
        yaim_config_file_set_value "${file2update}" "srmPort" "$SRMPORT"
    fi

    yaim_query_conf_node_dcap_door
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        yaim_query_conf_node_dcap_door_port
        DCAPPORT=$RET
        yaim_config_file_set_value "${file2update}" "dCapPort" "$DCAPPORT"
    fi

    yaim_query_conf_node_gsidcap_door
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        yaim_query_conf_node_gsidcap_door_port
        GSIDCAPPORT=$RET
        yaim_config_file_set_value "${file2update}" "dCapGsiPort" "$GSIDCAPPORT"
    fi
    
    yaim_query_conf_node_xrootd_door
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        yaim_query_conf_node_xrootd_door_port
        XROOTDPORT=$RET
        yaim_config_file_set_value "${file2update}" "xrootdPort" "$XROOTDPORT"
    fi
    yaim_query_conf_voms
    reusevoms=$?
    if [ "$reusevoms" == "1" ] ; then
        yaim_config_file_set_value "${file2update}" "useGPlazmaAuthorizationModule" "true"
        yaim_config_file_set_value "${file2update}" "useGPlazmaAuthorizationCell" "false"
    fi
    
    yaim_query_conf_node_dcache_name_type
    nameServerType=${RET}
    if [ "${nameServerType}" == "chimera" ] ; then
        yaim_config_file_set_value "${file2update}" PermissionHandlerDataSource diskCacheV111.services.PnfsManagerFileMetaDataSource
        yaim_config_file_set_value "${file2update}" cacheInfo pnfs
    fi
    
    
    yaim_config_file_update_done ${configFile}
    if [ $? == 1 ] ; then 
        yaim_state_service_restart_needed core
        yaimlog WARNING "Changed the file ${configFile}"
    fi
    
    
    
    
    yaimlog DEBUG "function yaim_config_dcache_files_key_value_dCacheSetup stop"
}

yaim_config_dcache_files_key_value_node_config()
{
    yaimlog DEBUG "function yaim_config_dcache_files_key_value_node_config start"
    local reusevoms
    local servicesettings
    configFile=/opt/d-cache/etc/node_config
    # Start processing
    yaim_config_file_update_prepare ${configFile}
    file2update=$RET
    yaim_query_conf_node_srm_door
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        servicesettings="${servicesettings} srm"
    fi

    yaim_query_conf_node_gsiftp_door
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        servicesettings="${servicesettings} gridftp"
    fi

    yaim_query_conf_node_dcap_door
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        servicesettings="${servicesettings} dcap"
    fi
 

    yaim_query_conf_node_gsidcap_door
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        servicesettings="${servicesettings} gsidcap"
    fi
    
    yaim_query_conf_node_xrootd_door
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        servicesettings="${servicesettings} xrootd"
    fi
    yaim_query_conf_infoprovider_system
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        servicesettings="${servicesettings} infoProvider"
    fi
    yaim_query_conf_info_system
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        servicesettings="${servicesettings} info"
    fi
    yaim_query_conf_web_system
    tmptrue=$?
    if [ "x${tmptrue}" == "x1" ] ;then
        servicesettings="${servicesettings} httpd"
    fi
      
    yaim_query_conf_node_dcache_admin
    admintrue=$?
    yaim_query_conf_node_dcache_name_server
    NameServerTrue=$?
    if [ "${admintrue}${NameServerTrue}" == "11" ] ;then
        yaim_config_file_set_value "${file2update}" "NODE_TYPE" "custom"
        servicesettings="${servicesettings} admin dir lm pnfs dCache utility"
    fi
    if [ "${admintrue}${NameServerTrue}" == "00" ] ;then
        yaim_config_file_set_value "${file2update}" "NODE_TYPE" "custom"
    fi
    if [ "${admintrue}${NameServerTrue}" == "01" ] ;then
        yaim_config_file_set_value "${file2update}" "NODE_TYPE" "custom"
        servicesettings="${servicesettings} dir pnfs"
    fi
    if [ "${admintrue}${NameServerTrue}" == "10" ] ;then
        yaim_config_file_set_value "${file2update}" "NODE_TYPE" "custom"
	servicesettings="${servicesettings} admin lm dCache utility"
    fi
    yaim_query_conf_node_dcache_name_server_list
    NameServerNode=$RET
    yaim_config_file_set_value "${file2update}" "ADMIN_NODE" "$NameServerNode"
    yaim_config_file_set_value "${file2update}" "NAMESPACE_NODE" "$NameServerNode"
    
    yaim_config_file_set_value "${file2update}" "SERVER_ID" "$MY_DOMAIN"
    
    yaim_query_conf_voms
    reusevoms=$?
    if [ "$reusevoms" == "1" ] ; then
        servicesettings="${servicesettings} gPlazma"
    fi
    
    yaim_query_conf_node_dcache_name_type
    nameServerType=${RET}
    if [ "${nameServerType}" == "chimera" ] ; then
        yaim_config_file_set_value "${file2update}" NAMESPACE chimera
	servicesettings="${servicesettings} chimera"
    fi
    yaim_config_file_set_value "${file2update}" "SERVICES" "${servicesettings}"
    # Finished editng values
    yaim_config_file_update_done ${configFile}
    if [ $? == 1 ] ; then 
        yaim_state_service_restart_needed core
        yaimlog WARNING "Changed the file ${configFile}"
    fi
    yaimlog DEBUG "function yaim_config_dcache_files_key_value_node_config stop"
}

yaim_config_dcache_files_key_value_srm_gplazma_policy()
{
  yaimlog DEBUG "function yaim_config_dcache_files_key_value_srm_gplazma_policy start"
  local configFile
  local file2update
  local reusevoms
  local reusekpwd
  
  configFile="/opt/d-cache/etc/dcachesrm-gplazma.policy"
  # Start processing
  yaim_config_file_update_prepare ${configFile}
  file2update=$RET
  yaim_query_conf_voms
  reusevoms=$?
  if [ "$reusevoms" == "1" ] ; then
    yaim_config_file_set_value "${file2update}" "gplazmalite-vorole-mapping" "\"ON\""
  else
    yaim_config_file_set_value "${file2update}" "gPlazmaService" "\"OFF\""
  fi
  
  yaim_query_conf_kpwd
  reusekpwd=$?
  if [ "$reusekpwd" == "1" ] ; then
    yaim_config_file_set_value "${file2update}" "kpwd" "\"ON\""
  else
    yaim_config_file_set_value "${file2update}" "kpwd" "\"OFF\""
  fi
  yaim_config_file_update_done ${configFile}
  if [ $? == 1 ] ; then 
    yaim_state_service_restart_needed core
    yaimlog WARNING "Changed the file ${configFile}"
  fi
  yaimlog DEBUG "function yaim_config_dcache_files_key_value_srm_gplazma_policy stop"
}

yaim_config_dcache_files_key_value()
{
    # This function calls all the functions that mod the d_Cache 
    # configuration to match that of Yaim
    yaimlog DEBUG "function yaim_config_dcache_files_key_value start"
    yaim_config_dcache_files_key_value_dCacheSetup
    yaim_config_dcache_files_key_value_node_config
    yaim_config_dcache_files_key_value_srm_gplazma_policy
    yaimlog DEBUG "function yaim_config_dcache_files_key_value stop"
}

yaim_config_dcache_validate_host_name()
{
    # Returns 0 when node is not valid FQDN
    # Returns 1 when node is valid FQDN
    local testname
    local transname
    testname=$1
    transname=${testname%.*}
    if [ "$testname" == "${transname}" ] ; then
        return 0
    else
        return 1
    fi
}

yaim_config_dcache_validate_hosts()
{
    yaimlog DEBUG "function yaim_config_dcache_validate_hosts start"
    local hoststocheck
    local poollist
    local ahost
    local retcode
    yaim_query_conf_dcache_pool_node_list
    poollist=$RET
    hoststocheck="${DCACHE_ADMIN} ${poollist}"
    for ahost in $(echo $hoststocheck)
    do
        yaim_config_dcache_validate_host_name $ahost
        retcode=$?
        if [ "$retcode" == "0" ] ; then
            echo "Error: Hostname '${ahost}' does not appear to be a FQDN"
            echo "Error: Please check your site-info.def file"
            exit 1
        fi
    done
    yaimlog DEBUG "function yaim_config_dcache_validate_hosts stop"
}

config_sedcache_check() 
{
  return 0
}

config_sedcache_run() 
{
  local rcadmin
  local rcclient
  local rcdoor
  local rcNameServer
  local rcdbserver
  local rcrmpgsql
  local rcrmdcconf
  local rcrmpnfs
  local rcisvd
  local reusevoms
  local reusekpwd
  if [ "yes" == "$DESYOVERRIDE" ] ; then
    return 0
  fi
  INSTALL_ROOT=${INSTALL_ROOT:-/opt}
  # Validate all inputs as best as possible
  yaim_config_dcache_validate_hosts
  # First Check to see if admin or pool node
  yaim_query_conf_node_dcache_admin
  rcadmin=$?
  yaim_query_conf_node_dcache_pool
  rcclient=$?
  yaim_query_conf_node_dcache_door
  rcdoor=$?
  yaim_query_conf_node_dcache_name_server
  rcNameServer=$?
  if [ "${rcadmin}${rcclient}${rcdoor}${rcNameServer}${rcNameServer}" == "00000" ]; then
    yaimlog ERROR "This host was not recognized as admin,pool or door"
    yaimlog ERROR "Please check variables DCACHE_ADMIN and DCACHE_POOL"
    yaimlog ERROR "or for door nodes the variables DCACHE_DOOR_SRM,"
    yaimlog ERROR "DCACHE_DOOR_GSIFTP,DCACHE_DOOR_GSIDCAP,DCACHE_DOOR_DCAP"    
    exit 1
  fi
  requires DCACHE_ADMIN DCACHE_POOLS
  #  requires USERS_CONF VOS
  #
  #  if [ ! -e $USERS_CONF ]; then
  #    echo "$USERS_CONF not found."
  #    exit 1
  #  fi
  #  check_users_conf_format
  config_sedcache_pgsql
  

  #Setup the init scripts
  config_dcache_config_system_files_init
  
  # Check to see if D-Cache config should be reinstalled  
  yaim_state_reset_dcache_config
  rcrmpnfs=$?
  if [ "$rcrmpnfs" == "1" ]
  then
    requires DCACHE_ADMIN DCACHE_POOLS
    config_dcache_config_files
    config_sedcache_check_sanity || exit $?
    config_dcache_config_system_files    
  else
    echo "Skipping D-Cache config Reset"
  fi
  yaim_config_dcache_files_key_value
  
  config_sechimera_run
  config_sepnfs_run
  config_gip_dcache_info_run
  config_dcache_config_dcache_scripts  
  config_dcache_pnfs_databases
  yaim_query_conf_voms
  reusevoms=$?
  if [ "$reusevoms" == "1" ] ; then
    config_dcache_vorolemap_authz_update
  fi
  yaim_query_conf_kpwd
  reusekpwd=$?
  if [ "$reusevoms" == "1" ] ; then
    config_dcache_kpwd_update
  fi
  if [ "${rcadmin}" == "1" ]; then
    yaim_config_pool_manager_add_lines
  fi
  config_dcache_config_pools
  yaim_state_service_restart_needed_do
  #Setup the Cron scripts
  config_sedcache_cron
  echo "D-cache takes some time to initialise. Please wait 8-10 Min's for the service to start."
  return 0

}

config_sedcache() 
{
  if [ "yes" == "$DESYOVERRIDE" ] ; then
    return 0
  fi
  config_sedcache_run
}

